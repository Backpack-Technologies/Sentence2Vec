{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        loop = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "            loop += 1\n",
    "            if loop % 10000 == 0:\n",
    "                print(loop)\n",
    "\n",
    "#             if loop == 1000000:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "documents = list(read_corpus('./data/dataTitle.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "    def on_train_begin(self, model):\n",
    "        print(\"train started\")\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_train_end(self, model):\n",
    "        print(\"train ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, min_count=2, epochs=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, min_count=2, epochs =10, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train started\n",
      "Epoch #0 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamdipta/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "train ended\n",
      "CPU times: user 51min 30s, sys: 9min 22s, total: 1h 52s\n",
      "Wall time: 21min 2s\n",
      "train started\n",
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "train ended\n",
      "CPU times: user 32min 35s, sys: 14min 52s, total: 47min 27s\n",
      "Wall time: 24min 40s\n"
     ]
    }
   ],
   "source": [
    "for index, model in enumerate(models):\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=self.epochs, callbacks=[EpochLogger()])\n",
    "    model.save('./test-model/data' + str(index) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model in enumerate(models):\n",
    "    models[index] = Doc2Vec.load('./model/data' + str(index) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get(model, st):\n",
    "    tokens = st.lower().split()\n",
    "    new_vector = model.infer_vector(tokens)\n",
    "    sims = model.docvecs.most_similar([new_vector])\n",
    "    for index, val in sims:\n",
    "        print(str(val) + \" \" + str(' '.join(documents[index].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc2,s0.001,t8)\n",
      "0.8697952032089233 neoprene pro carrying case sleeve for macbook air macbook pro macbook pro retina display black\n",
      "0.8628667593002319 uniharpa inch laptop sleeve pu case cover for macbook air retina macbook pro inch ipad pro black\n",
      "0.8584039807319641 us keyboard for macbook retina\n",
      "0.852268397808075 thule subterra macbook pro sleeve inch gray\n",
      "0.849841296672821 felt laptop sleeve for inch macbook air retina macbook pro inch ipad blue\n",
      "0.8488801717758179 thule gauntlet sleeve for macbook pro macbook pro with retina display blacks\n",
      "0.8471614122390747 mobility islice pro for macbook pro retina mid early red\n",
      "0.8420846462249756 tomtoc protective laptop sleeve bag for inch macbook air macbook pro retina surface book surface laptop ipad pro chromebook tablet red\n",
      "0.8413190841674805 tucano doppio backpack for macbook pro retina notebook black\n",
      "0.841266393661499 kengel plastic hard case cover for macbook pro retina inch and black\n",
      "\n",
      "\n",
      "\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc2,s0.001,t8)\n",
      "0.9027600288391113 la conciencia nacional puertorriquena spanish edition\n",
      "0.897063136100769 catherine malandrino women slip on loafer\n",
      "0.8900920748710632 benedicta gourmet creamy garlic sauce sauce aioli oz\n",
      "0.8894842267036438 carter boys pc playwear sets\n",
      "0.8870035409927368 uxcell pcs plastic straight connector fitting water hose pipe tube coupler adapter\n",
      "0.8852168321609497 homestyle chicken bone broth\n",
      "0.8848780989646912 wall decal\n",
      "0.8834033012390137 new balance men trail running shoe\n",
      "0.882003128528595 nike men basketball shorts\n",
      "0.8810867071151733 belts for men sliding ratchet dress belt with automatic buckle\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "guess_it = \"macbook pro retina\" \n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    get(model, guess_it)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "# Have to use dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models[0].docvecs.save_word2vec_format('./dbow.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"./data/dbow.txt\"\n",
    "DIMENSION_FILE = \"./data/dataDimesions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "with open(EMBEDDING_FILE, \"r\") as ip:\n",
    "    flg = 0\n",
    "    \n",
    "    for line in ip:\n",
    "        if flg:\n",
    "            data = line.split()\n",
    "            del data[0]\n",
    "            data = list(map(float, data))\n",
    "            X.append(data)\n",
    "        flg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "with open(DIMENSION_FILE, \"r\") as op:\n",
    "    for line in op:\n",
    "        data = line.split()\n",
    "        data = list(map(float, data))\n",
    "        Y.append(data)\n",
    "        if len(Y) == len(X):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 1000000\n",
      "200 4\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(Y))\n",
    "print(len(X[0]), len(Y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs = np.array(X)\n",
    "target = np.array(Y)\n",
    "\n",
    "ratio = int( docs.shape[0]/10 )\n",
    "X_train = docs[ratio:,:]\n",
    "X_test = docs[:ratio,:]\n",
    "y_train = target[ratio:,:]\n",
    "y_test = target[:ratio,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_size = 200\n",
    "generations = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 810000 samples, validate on 90000 samples\n",
      "Epoch 1/44\n",
      "810000/810000 [==============================] - 36s 44us/step - loss: 310.0006 - mean_absolute_error: 309.7186 - val_loss: 311.6571 - val_mean_absolute_error: 311.4698\n",
      "Epoch 2/44\n",
      "810000/810000 [==============================] - 38s 47us/step - loss: 306.1872 - mean_absolute_error: 305.9911 - val_loss: 311.0696 - val_mean_absolute_error: 310.8647\n",
      "Epoch 3/44\n",
      "810000/810000 [==============================] - 40s 49us/step - loss: 306.1307 - mean_absolute_error: 305.9253 - val_loss: 311.2237 - val_mean_absolute_error: 311.0166\n",
      "Epoch 4/44\n",
      "810000/810000 [==============================] - 37s 45us/step - loss: 306.0046 - mean_absolute_error: 305.8041 - val_loss: 310.2569 - val_mean_absolute_error: 310.0657\n",
      "Epoch 5/44\n",
      "810000/810000 [==============================] - 36s 44us/step - loss: 306.0001 - mean_absolute_error: 305.8159 - val_loss: 310.1602 - val_mean_absolute_error: 309.9906\n",
      "Epoch 6/44\n",
      "810000/810000 [==============================] - 37s 45us/step - loss: 305.7674 - mean_absolute_error: 305.6087 - val_loss: 310.5611 - val_mean_absolute_error: 310.4212\n",
      "Epoch 7/44\n",
      "810000/810000 [==============================] - 36s 45us/step - loss: 305.6198 - mean_absolute_error: 305.4928 - val_loss: 310.1687 - val_mean_absolute_error: 310.0527\n",
      "Epoch 8/44\n",
      "810000/810000 [==============================] - 37s 45us/step - loss: 305.4329 - mean_absolute_error: 305.3288 - val_loss: 310.3344 - val_mean_absolute_error: 310.2344\n",
      "Epoch 9/44\n",
      "810000/810000 [==============================] - 37s 46us/step - loss: 305.3050 - mean_absolute_error: 305.2196 - val_loss: 309.8441 - val_mean_absolute_error: 309.7696\n",
      "Epoch 10/44\n",
      "810000/810000 [==============================] - 36s 45us/step - loss: 305.2111 - mean_absolute_error: 305.1423 - val_loss: 310.1313 - val_mean_absolute_error: 310.0698\n",
      "Epoch 11/44\n",
      "810000/810000 [==============================] - 5613s 7ms/step - loss: 305.2276 - mean_absolute_error: 305.1617 - val_loss: 309.8281 - val_mean_absolute_error: 309.7596\n",
      "Epoch 12/44\n",
      "417792/810000 [==============>...............] - ETA: 19s - loss: 304.5806 - mean_absolute_error: 304.5219"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "activation_function = keras.layers.PReLU()\n",
    "\n",
    "# Input Layer\n",
    "model.add(keras.layers.Dense(405, kernel_initializer=keras.initializers.random_uniform, input_dim=doc_embedding_size,\n",
    "                             kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                             bias_regularizer=keras.regularizers.l2(0.01), name=\"input\"))\n",
    "model.add(activation_function)\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(keras.layers.Dense(405, kernel_initializer='normal', name=\"hidden1\"))\n",
    "model.add(activation_function)\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Dense(405, kernel_initializer='normal', name=\"hidden2\"))\n",
    "model.add(activation_function)\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(keras.layers.Dense(4, kernel_initializer='normal', name=\"output\", activation=\"relu\"))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=\"Adam\", metrics=['mean_absolute_error'])\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"./model/best.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                                 mode='auto')\n",
    "callbacks_list = []\n",
    "\n",
    "# model = create_neural_model()\n",
    "model.fit(X_train, y_train, epochs=generations, batch_size=256, validation_split=0.1, callbacks=callbacks_list)\n",
    "print(\"Current one: \", model.evaluate(X_test, y_test, batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
