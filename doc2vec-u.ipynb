{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cores\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        loop = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                word_list = nltk.tokenize.word_tokenize(line)\n",
    "                word_list2 = [w.strip() for w in word_list if w.strip() not in stop_words]\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\" \".join(word_list2)), [i])\n",
    "\n",
    "            loop += 1\n",
    "            if loop % 10000 == 0:\n",
    "                print(loop)\n",
    "\n",
    "            if loop == 1000000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "documents = list(read_corpus('./data/dataTitle.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "    def on_train_begin(self, model):\n",
    "        print(\"train started\")\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_train_end(self, model):\n",
    "        print(\"train ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, min_count=2, epochs=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "#     Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, min_count=2, epochs =10, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc2,s0.001,t16)\n"
     ]
    }
   ],
   "source": [
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "# models[1].reset_from(models[0])\n",
    "# print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train started\n",
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "train ended\n",
      "CPU times: user 1h 11min 46s, sys: 17min 4s, total: 1h 28min 50s\n",
      "Wall time: 32min 32s\n"
     ]
    }
   ],
   "source": [
    "for index, model in enumerate(models):\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=10, callbacks=[EpochLogger()])\n",
    "    model.save('./data/DbowModel' + str(index) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, model in enumerate(models):\n",
    "    models[index] = Doc2Vec.load('./data/DbowModel' + str(index) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get(model, st):\n",
    "    tokens = st.lower().split()\n",
    "    new_vector = model.infer_vector(tokens)\n",
    "    sims = model.docvecs.most_similar([new_vector], topn=20)\n",
    "    for index, val in sims:\n",
    "        print(str(' '.join(documents[index].words)) + \" - \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc2,s0.001,t16)\n",
      "ninewest expandable spinner luggage - 0.78464674949646\n",
      "nautica hardside expandable spinner luggage - 0.7668018341064453\n",
      "swissgear baden expandable spinner suitcase silver - 0.7467473149299622\n",
      "swissgear meyrin expandable spinner suitcase dark blue - 0.7464632391929626\n",
      "swissgear travel gear expandable hardside spinner luggage - 0.7431485652923584\n",
      "swissgear baden expandable spinner suitcase silver - 0.7429184317588806\n",
      "avolve expandable spinner black one size - 0.7416092157363892\n",
      "steve madden luggage expandable softside suitcase with spinner wheels - 0.7402724623680115\n",
      "avolve expandable spinner black one size - 0.7398602366447449\n",
      "nautica carry on expandable spinner luggage - 0.7389228940010071\n",
      "luggage quilte lightweight expandable spinner patriot blue - 0.7364516258239746\n",
      "nautica carry on expandable spinner luggage - 0.7327922582626343\n",
      "luggage duotone wheel luggage spinner - 0.7274551391601562\n",
      "kemyer lightweight pc expandable hardside spinner luggage spinner set - 0.7273848056793213\n",
      "steve madden luggage large expandable softside suitcase with spinner wheels - 0.7263124585151672\n",
      "alex luggage suitcase hardside spinner trolley expandable tsa - 0.7262651324272156\n",
      "lucas luggage hard case expandable suitcase with spinner wheels - 0.724585771560669\n",
      "travelers choice cornwall lightweight expandable upright spinner luggage - 0.7243712544441223\n",
      "kemyer lightweight pc expandable hardside spinner luggage spinner set - 0.7229146957397461\n",
      "steve madden luggage carry on expandable softside suitcase with spinner wheels - 0.7190980911254883\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "guess_it = \"Travelpro Luggage Crew 11 21 Carry-on Expandable Spinner w/Suiter and USB Port, Black\" \n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    get(model, guess_it)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "# Have to use dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models[0].docvecs.save_word2vec_format('./data/dbow.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"./data/dbow.txt\"\n",
    "DIMENSION_FILE = \"./data/dataDimesions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "with open(EMBEDDING_FILE, \"r\") as ip:\n",
    "    flg = 0\n",
    "    \n",
    "    for line in ip:\n",
    "        if flg:\n",
    "            data = line.split()\n",
    "            del data[0]\n",
    "            data = [float(i) for i in data]\n",
    "#             npd = np.array(data, dtype=float)\n",
    "#             print(npd)\n",
    "            X.append(data)\n",
    "#             X = np.concatenate(X, npd)\n",
    "            \n",
    "            if flg == 1000000:\n",
    "                break\n",
    "        flg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(X)\n",
    "# np.asarray(X).shape\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "with open(DIMENSION_FILE, \"r\") as op:\n",
    "    for line in op:\n",
    "        data = line.split()\n",
    "        data = list(map(float, data))\n",
    "        Y.append(data)\n",
    "        if len(Y) == len(X):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 1000000\n",
      "200 4\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(Y))\n",
    "print(len(X[0]), len(Y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 200) (100000, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# print(X[:2])\n",
    "# print(len(X), len(X[0]))\n",
    "# print(len(Y), len(Y[0]))\n",
    "# docs = np.array(X[i] for i in X)\n",
    "docs = np.array([i for i in X])\n",
    "target = np.array(Y)\n",
    "\n",
    "# print(docs[:2])\n",
    "# print(target[:2])\n",
    "# docs.reshape([1100001, 200])\n",
    "print(docs.shape, target.shape)\n",
    "ratio = int( docs.shape[0]/10 )\n",
    "X_train = docs[ratio:]\n",
    "X_test = docs[:ratio]\n",
    "y_train = target[ratio:,:]\n",
    "y_test = target[:ratio,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_embedding_size = 200\n",
    "generations = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81000 samples, validate on 9000 samples\n",
      "Epoch 1/10000\n",
      "81000/81000 [==============================] - 4s 50us/step - loss: 311.7402 - mean_absolute_error: 310.6904 - val_loss: 231.6713 - val_mean_absolute_error: 230.8048\n",
      "Epoch 2/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 255.1163 - mean_absolute_error: 254.1193 - val_loss: 212.7802 - val_mean_absolute_error: 211.6787\n",
      "Epoch 3/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 246.9152 - mean_absolute_error: 245.7516 - val_loss: 209.6385 - val_mean_absolute_error: 208.4286\n",
      "Epoch 4/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 243.9210 - mean_absolute_error: 242.6744 - val_loss: 210.0077 - val_mean_absolute_error: 208.7315\n",
      "Epoch 5/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 241.6919 - mean_absolute_error: 240.3890 - val_loss: 211.9230 - val_mean_absolute_error: 210.5900\n",
      "Epoch 6/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 239.7562 - mean_absolute_error: 238.3950 - val_loss: 204.9065 - val_mean_absolute_error: 203.5297\n",
      "Epoch 7/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 238.3891 - mean_absolute_error: 236.9897 - val_loss: 203.5082 - val_mean_absolute_error: 202.0876\n",
      "Epoch 8/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 236.7953 - mean_absolute_error: 235.3486 - val_loss: 204.0535 - val_mean_absolute_error: 202.5744\n",
      "Epoch 9/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 236.0508 - mean_absolute_error: 234.5394 - val_loss: 202.5380 - val_mean_absolute_error: 200.9953\n",
      "Epoch 10/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 234.4450 - mean_absolute_error: 232.8772 - val_loss: 201.7226 - val_mean_absolute_error: 200.1311\n",
      "Epoch 11/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 233.4508 - mean_absolute_error: 231.8244 - val_loss: 200.3478 - val_mean_absolute_error: 198.6920\n",
      "Epoch 12/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 232.0717 - mean_absolute_error: 230.3865 - val_loss: 200.9535 - val_mean_absolute_error: 199.2298\n",
      "Epoch 13/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 230.8628 - mean_absolute_error: 229.1101 - val_loss: 200.5517 - val_mean_absolute_error: 198.7615\n",
      "Epoch 14/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 229.8105 - mean_absolute_error: 227.9903 - val_loss: 202.7463 - val_mean_absolute_error: 200.8961\n",
      "Epoch 15/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 228.4915 - mean_absolute_error: 226.6056 - val_loss: 199.0469 - val_mean_absolute_error: 197.1290\n",
      "Epoch 16/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 227.4906 - mean_absolute_error: 225.5441 - val_loss: 198.0802 - val_mean_absolute_error: 196.1055\n",
      "Epoch 17/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 226.7232 - mean_absolute_error: 224.7197 - val_loss: 198.4018 - val_mean_absolute_error: 196.3724\n",
      "Epoch 18/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 225.7603 - mean_absolute_error: 223.7042 - val_loss: 198.4594 - val_mean_absolute_error: 196.3765\n",
      "Epoch 19/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 224.9011 - mean_absolute_error: 222.7893 - val_loss: 196.7600 - val_mean_absolute_error: 194.6201\n",
      "Epoch 20/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 223.8852 - mean_absolute_error: 221.7134 - val_loss: 198.2989 - val_mean_absolute_error: 196.0979\n",
      "Epoch 21/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 223.7338 - mean_absolute_error: 221.4999 - val_loss: 196.9296 - val_mean_absolute_error: 194.6592\n",
      "Epoch 22/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 222.4874 - mean_absolute_error: 220.1851 - val_loss: 197.7189 - val_mean_absolute_error: 195.3848\n",
      "Epoch 23/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 221.6965 - mean_absolute_error: 219.3301 - val_loss: 196.0115 - val_mean_absolute_error: 193.6179\n",
      "Epoch 24/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 220.8436 - mean_absolute_error: 218.4155 - val_loss: 195.7083 - val_mean_absolute_error: 193.2520\n",
      "Epoch 25/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 220.1505 - mean_absolute_error: 217.6568 - val_loss: 196.1994 - val_mean_absolute_error: 193.6755\n",
      "Epoch 26/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 219.7072 - mean_absolute_error: 217.1447 - val_loss: 197.9096 - val_mean_absolute_error: 195.3093\n",
      "Epoch 27/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 218.9130 - mean_absolute_error: 216.2757 - val_loss: 195.9447 - val_mean_absolute_error: 193.2772\n",
      "Epoch 28/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 218.0981 - mean_absolute_error: 215.3899 - val_loss: 195.3294 - val_mean_absolute_error: 192.5891\n",
      "Epoch 29/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 217.7949 - mean_absolute_error: 215.0205 - val_loss: 196.3022 - val_mean_absolute_error: 193.4909\n",
      "Epoch 30/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 216.9125 - mean_absolute_error: 214.0633 - val_loss: 194.7837 - val_mean_absolute_error: 191.9063\n",
      "Epoch 31/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 215.8300 - mean_absolute_error: 212.9135 - val_loss: 194.8679 - val_mean_absolute_error: 191.9149\n",
      "Epoch 32/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 215.5552 - mean_absolute_error: 212.5689 - val_loss: 193.6394 - val_mean_absolute_error: 190.6271\n",
      "Epoch 33/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 214.4581 - mean_absolute_error: 211.4080 - val_loss: 194.6060 - val_mean_absolute_error: 191.5229\n",
      "Epoch 34/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 214.0516 - mean_absolute_error: 210.9281 - val_loss: 195.1964 - val_mean_absolute_error: 192.0449\n",
      "Epoch 35/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 213.8176 - mean_absolute_error: 210.6309 - val_loss: 193.3094 - val_mean_absolute_error: 190.0831\n",
      "Epoch 36/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 212.8879 - mean_absolute_error: 209.6190 - val_loss: 197.0319 - val_mean_absolute_error: 193.7353\n",
      "Epoch 37/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 212.2127 - mean_absolute_error: 208.8747 - val_loss: 194.3124 - val_mean_absolute_error: 190.9416\n",
      "Epoch 38/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 211.8526 - mean_absolute_error: 208.4428 - val_loss: 196.5607 - val_mean_absolute_error: 193.1203\n",
      "Epoch 39/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 211.0919 - mean_absolute_error: 207.6192 - val_loss: 197.6729 - val_mean_absolute_error: 194.1647\n",
      "Epoch 40/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 210.7909 - mean_absolute_error: 207.2467 - val_loss: 195.2982 - val_mean_absolute_error: 191.7189\n",
      "Epoch 41/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 210.1172 - mean_absolute_error: 206.4958 - val_loss: 193.5886 - val_mean_absolute_error: 189.9377\n",
      "Epoch 42/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 209.2286 - mean_absolute_error: 205.5375 - val_loss: 197.1404 - val_mean_absolute_error: 193.4139\n",
      "Epoch 43/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 208.8413 - mean_absolute_error: 205.0669 - val_loss: 194.0807 - val_mean_absolute_error: 190.2714\n",
      "Epoch 44/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 208.0195 - mean_absolute_error: 204.1675 - val_loss: 194.1175 - val_mean_absolute_error: 190.2192\n",
      "Epoch 45/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 207.7212 - mean_absolute_error: 203.7828 - val_loss: 193.9393 - val_mean_absolute_error: 189.9676\n",
      "Epoch 46/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 207.0848 - mean_absolute_error: 203.0665 - val_loss: 195.6048 - val_mean_absolute_error: 191.5578\n",
      "Epoch 47/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 206.4517 - mean_absolute_error: 202.3617 - val_loss: 198.1876 - val_mean_absolute_error: 194.0608\n",
      "Epoch 48/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 205.7185 - mean_absolute_error: 201.5542 - val_loss: 196.9956 - val_mean_absolute_error: 192.7939\n",
      "Epoch 49/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 205.2891 - mean_absolute_error: 201.0434 - val_loss: 196.3949 - val_mean_absolute_error: 192.1120\n",
      "Epoch 50/10000\n",
      "81000/81000 [==============================] - 2s 19us/step - loss: 204.4360 - mean_absolute_error: 200.1169 - val_loss: 193.3618 - val_mean_absolute_error: 189.0077\n",
      "Epoch 51/10000\n",
      "81000/81000 [==============================] - 1s 18us/step - loss: 204.2287 - mean_absolute_error: 199.8279 - val_loss: 194.5776 - val_mean_absolute_error: 190.1440\n",
      "10000/10000 [==============================] - 0s 8us/step\n",
      "Current one:  [239.32831604003906, 234.89468198242187]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "activation_function = keras.layers.PReLU()\n",
    "\n",
    "# Input Layer\n",
    "model.add(keras.layers.Dense(405, kernel_initializer=keras.initializers.random_uniform, input_dim=doc_embedding_size,\n",
    "                             kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                             bias_regularizer=keras.regularizers.l2(0.01), name=\"input\"))\n",
    "model.add(activation_function)\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(keras.layers.Dense(405, kernel_initializer='normal', name=\"hidden1\"))\n",
    "model.add(activation_function)\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Dense(405, kernel_initializer='normal', name=\"hidden2\"))\n",
    "model.add(activation_function)\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(keras.layers.Dense(4, kernel_initializer='normal', name=\"output\", activation=\"relu\"))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=\"Adam\", metrics=['mean_absolute_error'])\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"./temp/best.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                                 mode='auto')\n",
    "\n",
    "earlyStopping = keras.callbacks.EarlyStopping(patience=16, mode=\"auto\")\n",
    "callbacks_list = [earlyStopping]\n",
    "\n",
    "# model = create_neural_model()\n",
    "model.fit(X_train, y_train, epochs=generations, batch_size=256, validation_split=0.1, callbacks=callbacks_list)\n",
    "print(\"Current one: \", model.evaluate(X_test, y_test, batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
